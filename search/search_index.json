{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Multi Cloud Data Security Workshop Welcome to the Multi Cloud Data Security Workshop! This workbook provides: Workshop Guides Additional Resources This workshop is based on content from SANS Institute SEC510: Cloud Security Controls and Mitigations . SEC510 provides cloud security analysts, engineers, and researchers with practical security controls that can help organizations reduce their attack surface and prevent security incidents from becoming breaches. To learn more, please visit here , review the syllabus, and click the Course Demo button for a free ~1 hour module. Getting Started Click here to get started! SANS Cloud Security Curriculum SANS Cloud Security seeks to ingrain security into the minds of every organization building and monitoring resources in the public cloud. Visit the Resources section of this workbook to access resources from SANS Cloud Security that are relevant to the subjects covered in this workshop. Interested in contributing your Cloud or DevSecOps success stories to the SANS Blog ? Let us know and we'll share them with the community.","title":"Home"},{"location":"#multi-cloud-data-security-workshop","text":"Welcome to the Multi Cloud Data Security Workshop! This workbook provides: Workshop Guides Additional Resources This workshop is based on content from SANS Institute SEC510: Cloud Security Controls and Mitigations . SEC510 provides cloud security analysts, engineers, and researchers with practical security controls that can help organizations reduce their attack surface and prevent security incidents from becoming breaches. To learn more, please visit here , review the syllabus, and click the Course Demo button for a free ~1 hour module.","title":"Multi Cloud Data Security Workshop"},{"location":"#getting-started","text":"Click here to get started!","title":"Getting Started"},{"location":"#sans-cloud-security-curriculum","text":"SANS Cloud Security seeks to ingrain security into the minds of every organization building and monitoring resources in the public cloud. Visit the Resources section of this workbook to access resources from SANS Cloud Security that are relevant to the subjects covered in this workshop. Interested in contributing your Cloud or DevSecOps success stories to the SANS Blog ? Let us know and we'll share them with the community.","title":"SANS Cloud Security Curriculum"},{"location":"resources/","text":"Resources Check out these resources from SANS Cloud Security that are relevant to the subjects covered in this workshop: Secure Service Configuration in AWS, Azure, & GCP Poster Multicloud CLI Cheat Sheet Cloud Ace Podcast \u2013 Season 1 hosted by workshop author Brandon Evans SANS 2023 Multicloud Survey: Navigating the Complexities of Multiple Clouds Cloud Security: Making Cloud Environments a Safer Place eBook Cloud Security Foundations, Frameworks, and Beyond This workshop is based on content from SANS Institute SEC510: Cloud Security Controls and Mitigations . SEC510 provides cloud security analysts, engineers, and researchers with practical security controls that can help organizations reduce their attack surface and prevent security incidents from becoming breaches. Check out some of our testimonials: To learn more, please visit here , review the syllabus, and click the Course Demo button for a free ~1 hour module.","title":"Resources"},{"location":"resources/#resources","text":"Check out these resources from SANS Cloud Security that are relevant to the subjects covered in this workshop: Secure Service Configuration in AWS, Azure, & GCP Poster Multicloud CLI Cheat Sheet Cloud Ace Podcast \u2013 Season 1 hosted by workshop author Brandon Evans SANS 2023 Multicloud Survey: Navigating the Complexities of Multiple Clouds Cloud Security: Making Cloud Environments a Safer Place eBook Cloud Security Foundations, Frameworks, and Beyond This workshop is based on content from SANS Institute SEC510: Cloud Security Controls and Mitigations . SEC510 provides cloud security analysts, engineers, and researchers with practical security controls that can help organizations reduce their attack surface and prevent security incidents from becoming breaches. Check out some of our testimonials: To learn more, please visit here , review the syllabus, and click the Course Demo button for a free ~1 hour module.","title":"Resources"},{"location":"workshop/","text":"Getting Started In this workshop, you will: Setup your own sandbox cloud accounts Discover the many ways that cloud data can be exfiltrated without the use of public storage buckets Examine the various data classification services that are available in the Big 3 CSPs Identify and remediate instances of Publicly Identifiable Information (PII) being inappropriately handled in a real lab environment Generate Sample Data Before we can explore the cloud data security services, we need some data to secure. We will be generating fake data with Mockaroo . Navigate to Mockaroo and generate our first file by clicking Generate Data : This will generate a file called MOCK_DATA.csv . Next, let us add some additional fields of our choosing and generate another file. Choose some sensitive fields (e.g., Diagnosis Code, EIN, etc.) and some potentially insensitive fields (e.g., Movie Title, Movie Genres, etc.), then click Generate Data to download another CSV file: It wouldn't be a workshop in the modern era if we didn't somehow include AI! \ud83d\ude0a As of April 12 th , 2023, Mockaroo supports the generation of fields on any topic using AI. Click Generate Fields Using AI to try this out: Enter a topic of your choice (e.g., Cybersecurity) and click Replace Existing Fields : Then click Generate Data to download another CSV file: Feel free to generate as many of these files as you would like. Select your Cloud(s) As this is a multi cloud data security workshop, we will be working with both Amazon Web Services (AWS) , Microsoft Azure , and Google Cloud . You can complete this workshop with as many or as few cloud providers as you would like! Select the cloud provider you would like to get started with: AWS Azure Google Cloud Then, from the sidebar, switch to another cloud provider as desired!","title":"Getting Started"},{"location":"workshop/#getting-started","text":"In this workshop, you will: Setup your own sandbox cloud accounts Discover the many ways that cloud data can be exfiltrated without the use of public storage buckets Examine the various data classification services that are available in the Big 3 CSPs Identify and remediate instances of Publicly Identifiable Information (PII) being inappropriately handled in a real lab environment","title":"Getting Started"},{"location":"workshop/#generate-sample-data","text":"Before we can explore the cloud data security services, we need some data to secure. We will be generating fake data with Mockaroo . Navigate to Mockaroo and generate our first file by clicking Generate Data : This will generate a file called MOCK_DATA.csv . Next, let us add some additional fields of our choosing and generate another file. Choose some sensitive fields (e.g., Diagnosis Code, EIN, etc.) and some potentially insensitive fields (e.g., Movie Title, Movie Genres, etc.), then click Generate Data to download another CSV file: It wouldn't be a workshop in the modern era if we didn't somehow include AI! \ud83d\ude0a As of April 12 th , 2023, Mockaroo supports the generation of fields on any topic using AI. Click Generate Fields Using AI to try this out: Enter a topic of your choice (e.g., Cybersecurity) and click Replace Existing Fields : Then click Generate Data to download another CSV file: Feel free to generate as many of these files as you would like.","title":"Generate Sample Data"},{"location":"workshop/#select-your-clouds","text":"As this is a multi cloud data security workshop, we will be working with both Amazon Web Services (AWS) , Microsoft Azure , and Google Cloud . You can complete this workshop with as many or as few cloud providers as you would like! Select the cloud provider you would like to get started with: AWS Azure Google Cloud Then, from the sidebar, switch to another cloud provider as desired!","title":"Select your Cloud(s)"},{"location":"workshop/aws/","text":"AWS Initialize an AWS Account Important Even when using a free-tier cloud account, AWS charges a small amount for some of the resources we will be creating in this workshop. You should expect a negligible bill for the resources you create in this workshop. To guarantee that additional bills will not accumulate, consider closing your account at the conclusion of the workshop. You must not use a cloud account with existing personal or corporate resources. We will be uploading sensitive files that should not be in production environments. Browse to the AWS Web Console . If you have not created an account already, sign up for a new account now. Press the Create a New AWS Account button. Fill out the form to sign up for a new free tier account. Sign in with the root account's username (email address or mobile number) and password, and then press the sign in button. After signing in, the console will redirect to the home screen where you can browse the available AWS services. Create an S3 Bucket and Upload the Mock Data In the AWS Console, type S3 in the search bar and click the first result: Click Create bucket : Give the bucket a globally unique name, such as sans-multicloud-data-workshop-**YOUR_NAME** . If the resulting bucket name is more than 63 characters, please abbreviate: Scroll to the bottom of the page, do not change any settings , and click Create bucket : Click on the newly created bucket: Click Upload : Click Add files , select the MOCK_DATA*.csv files generated earlier, and click Upload : Create a Macie Job In the AWS Console, type Macie in the search bar and click the first result: Click Create job : Refresh the results until you see your newly created bucket. Then, select it and click Next : Click Next again to go to Step 3. In a real environment, we would likely want to continuously audit for sensitive data on a schedule. However, for this workshop, we only need to scan on-demand. Select One-time job and click Next : Click Next three more times without changing any settings . Then, give the job any name you want (e.g., Test), click Next , scroll down to the bottom of the page, and click Submit : This job will take roughly 10-15 minutes to complete. Refresh periodically until the Status goes from Active (Running) to Complete . If desired, you can move onto another cloud provider in the workshop in the meantime. Then, click Findings on the sidebar: Click on one of the findings and look at the sensitive data detected: Click on the number next to Name under Personal information to see which cells it flagged as containing names. We can make a couple of interesting conclusions based on these results: Despite the fact that our first two spreadsheets contain 1,000 full names, Macie found far fewer . Macie only believes that some of these \"names\" are names based on the names it has seen previously. For example, we found that it detected the common name \"Ian\", but not the less common name \"Elsy\". This indicates that Macie may be less effective at finding names not commonly used in the United States. In fact, Amazon Macie's documentation states that it is \" limited to Latin character sets \". Macie appears to be evaluating each chunk of data independently of the context. Given that the second column is called \"first_name\", it is obvious to a human that the entire column should be treated at least as sensitively as a first name. Macie is not considering the \"email\" values to be sensitive. Most critically, Macie is not flagging ICD-10 Diagnosis Codes as sensitive . While these codes are not considered Personal Health Information (PHI) by itself , coupled with names, this information is highly private. Refine the Job Let us create a custom identifier to detect the ICD-10 Diagnosis Codes in our second spreadsheet. Create a new job, repeating all of the same steps until reaching Step 4 . Then, click Manage custom identifiers : In the newly opened window , click Create to create a new custom data identifier: Give the custom data identifier the name ICD-10 Diagnosis Code and paste the following ( Source ) into the Regular expression field: [A-TV-Z][0-9][0-9AB]\\.?[0-9A-TV-Z]{0,4} To confirm our RegEx works, paste these five CSV records into the Sample data section and hit Test id,first_name,last_name,email,gender,ip_address,diagnosis_code,ein,favorite_movie,favorite_movie_genre 1,Moshe,Tolefree,mtolefree0@imageshack.us,Male,111.207.126.6,G3185,49-6935923,A Flintstones Christmas Carol,Animation|Children|Comedy 2,Jacqui,Harbour,jharbour1@home.pl,Non-binary,152.80.84.54,T43693,44-6915050,Forgotten Silver,Comedy|Documentary 3,Koo,Readitt,kreaditt2@tripadvisor.com,Female,25.241.0.38,S061X2A,49-5315541,\"Alamo, The\",Drama|War|Western 4,Annetta,Moultrie,amoultrie3@msu.edu,Female,214.224.120.104,H6123,62-5428600,Nine Ways to Approach Helsinki (Yhdeks\u00e4n tapaa l\u00e4hesty\u00e4 Helsinki\u00e4),Documentary 5,Oralie,Halversen,ohalversen4@networksolutions.com,Female,239.220.166.49,S52501S,79-7959398,Those Awful Hats,Comedy You should get 5 matches . Then, click Submit : Return to the original window . Hit the refresh button, select the newly created identifier, and click Next : Click Next two more times without changing any settings , give the job any name, click Next , scroll down, and click Submit . This job will also take roughly 10-15 minutes to complete. Refresh periodically until the Status goes from Active (Running) to Complete . If desired, you can move onto another cloud provider in the workshop in the meantime. Then, analyze the findings like before. Did you get the results you expected? Terminate Cloud Resources and Instance Disable Macie In the AWS Console, return to Macie and click Settings : Scroll to the bottom of the page, click Disable Macie , and confirm: Delete S3 Bucket In the AWS Console, return to S3, select the bucket used in this workshop, Empty it, then Delete it: Close Account To guarantee that additional bills will not accumulate, consider closing your account at the conclusion of the workshop. Alternatively, feel free to keep this account for future testing.","title":"AWS"},{"location":"workshop/aws/#aws","text":"","title":"AWS"},{"location":"workshop/aws/#initialize-an-aws-account","text":"Important Even when using a free-tier cloud account, AWS charges a small amount for some of the resources we will be creating in this workshop. You should expect a negligible bill for the resources you create in this workshop. To guarantee that additional bills will not accumulate, consider closing your account at the conclusion of the workshop. You must not use a cloud account with existing personal or corporate resources. We will be uploading sensitive files that should not be in production environments. Browse to the AWS Web Console . If you have not created an account already, sign up for a new account now. Press the Create a New AWS Account button. Fill out the form to sign up for a new free tier account. Sign in with the root account's username (email address or mobile number) and password, and then press the sign in button. After signing in, the console will redirect to the home screen where you can browse the available AWS services.","title":"Initialize an AWS Account"},{"location":"workshop/aws/#create-an-s3-bucket-and-upload-the-mock-data","text":"In the AWS Console, type S3 in the search bar and click the first result: Click Create bucket : Give the bucket a globally unique name, such as sans-multicloud-data-workshop-**YOUR_NAME** . If the resulting bucket name is more than 63 characters, please abbreviate: Scroll to the bottom of the page, do not change any settings , and click Create bucket : Click on the newly created bucket: Click Upload : Click Add files , select the MOCK_DATA*.csv files generated earlier, and click Upload :","title":"Create an S3 Bucket and Upload the Mock Data"},{"location":"workshop/aws/#create-a-macie-job","text":"In the AWS Console, type Macie in the search bar and click the first result: Click Create job : Refresh the results until you see your newly created bucket. Then, select it and click Next : Click Next again to go to Step 3. In a real environment, we would likely want to continuously audit for sensitive data on a schedule. However, for this workshop, we only need to scan on-demand. Select One-time job and click Next : Click Next three more times without changing any settings . Then, give the job any name you want (e.g., Test), click Next , scroll down to the bottom of the page, and click Submit : This job will take roughly 10-15 minutes to complete. Refresh periodically until the Status goes from Active (Running) to Complete . If desired, you can move onto another cloud provider in the workshop in the meantime. Then, click Findings on the sidebar: Click on one of the findings and look at the sensitive data detected: Click on the number next to Name under Personal information to see which cells it flagged as containing names. We can make a couple of interesting conclusions based on these results: Despite the fact that our first two spreadsheets contain 1,000 full names, Macie found far fewer . Macie only believes that some of these \"names\" are names based on the names it has seen previously. For example, we found that it detected the common name \"Ian\", but not the less common name \"Elsy\". This indicates that Macie may be less effective at finding names not commonly used in the United States. In fact, Amazon Macie's documentation states that it is \" limited to Latin character sets \". Macie appears to be evaluating each chunk of data independently of the context. Given that the second column is called \"first_name\", it is obvious to a human that the entire column should be treated at least as sensitively as a first name. Macie is not considering the \"email\" values to be sensitive. Most critically, Macie is not flagging ICD-10 Diagnosis Codes as sensitive . While these codes are not considered Personal Health Information (PHI) by itself , coupled with names, this information is highly private.","title":"Create a Macie Job"},{"location":"workshop/aws/#refine-the-job","text":"Let us create a custom identifier to detect the ICD-10 Diagnosis Codes in our second spreadsheet. Create a new job, repeating all of the same steps until reaching Step 4 . Then, click Manage custom identifiers : In the newly opened window , click Create to create a new custom data identifier: Give the custom data identifier the name ICD-10 Diagnosis Code and paste the following ( Source ) into the Regular expression field: [A-TV-Z][0-9][0-9AB]\\.?[0-9A-TV-Z]{0,4} To confirm our RegEx works, paste these five CSV records into the Sample data section and hit Test id,first_name,last_name,email,gender,ip_address,diagnosis_code,ein,favorite_movie,favorite_movie_genre 1,Moshe,Tolefree,mtolefree0@imageshack.us,Male,111.207.126.6,G3185,49-6935923,A Flintstones Christmas Carol,Animation|Children|Comedy 2,Jacqui,Harbour,jharbour1@home.pl,Non-binary,152.80.84.54,T43693,44-6915050,Forgotten Silver,Comedy|Documentary 3,Koo,Readitt,kreaditt2@tripadvisor.com,Female,25.241.0.38,S061X2A,49-5315541,\"Alamo, The\",Drama|War|Western 4,Annetta,Moultrie,amoultrie3@msu.edu,Female,214.224.120.104,H6123,62-5428600,Nine Ways to Approach Helsinki (Yhdeks\u00e4n tapaa l\u00e4hesty\u00e4 Helsinki\u00e4),Documentary 5,Oralie,Halversen,ohalversen4@networksolutions.com,Female,239.220.166.49,S52501S,79-7959398,Those Awful Hats,Comedy You should get 5 matches . Then, click Submit : Return to the original window . Hit the refresh button, select the newly created identifier, and click Next : Click Next two more times without changing any settings , give the job any name, click Next , scroll down, and click Submit . This job will also take roughly 10-15 minutes to complete. Refresh periodically until the Status goes from Active (Running) to Complete . If desired, you can move onto another cloud provider in the workshop in the meantime. Then, analyze the findings like before. Did you get the results you expected?","title":"Refine the Job"},{"location":"workshop/aws/#terminate-cloud-resources-and-instance","text":"","title":"Terminate Cloud Resources and Instance"},{"location":"workshop/aws/#disable-macie","text":"In the AWS Console, return to Macie and click Settings : Scroll to the bottom of the page, click Disable Macie , and confirm:","title":"Disable Macie"},{"location":"workshop/aws/#delete-s3-bucket","text":"In the AWS Console, return to S3, select the bucket used in this workshop, Empty it, then Delete it:","title":"Delete S3 Bucket"},{"location":"workshop/aws/#close-account","text":"To guarantee that additional bills will not accumulate, consider closing your account at the conclusion of the workshop. Alternatively, feel free to keep this account for future testing.","title":"Close Account"},{"location":"workshop/azure/","text":"Azure Initialize an Azure Subscription Important You must not use a cloud account with existing personal or corporate resources. We will be uploading sensitive files that should not be in production environments. Browse to the Azure Portal . Login to an existing Microsoft account or create a new one: Dismiss any pop-up boxes in the portal until you see a listing of Azure services. Click Start under Start with an Azure free trial and follow all of the prompts until you are back at the portal: Create a Storage Account and Upload the Mock Data In the Azure Portal, type Storage accounts in the search bar and click the first result under Services : Click Create storage account : Click Create new to create a new resource group and give it any name that you would like. Then, give the storage account a globally unique name, such as sansworkshop**YOUR_NAME** . If the resulting bucket name is more than 24 characters, please abbreviate. Then, click Review : After Azure finishes running its final validation, click Create : Wait for the deployment to complete. Then, click Go to resource : Click Containers : Click + Container , give the container any name that you would like, make sure that Public access level is set to Private (no anonymous access) , and click Create : Click on the newly created container. Then, click Upload , Browse for files and select the MOCK_DATA*.csv files generated earlier: Then, click Upload : Generate and Use an Azure Storage Shared Access Signatures (SAS) Close the upload file modal. Then, click the ... next to one of the uploaded files and click Generate SAS : Without changing any settings, click **Generate SAS token and URL : Scroll down and view the Blob SAS token . It consists of several components: se ( signedexpiry ) - The date this token will expire . sp ( signedpermissions ) - The permission(s) granted by the token . For blobs, this is one or more of the following: r (Read), a (Add), c (Create), w (Write), d (Delete), and l (List). sv ( signedversion ) - The release date of this version of the storage service . sr ( signedresource ) - The type of resource that can be accessed with this SAS . In this case, it is b to indicate that it is for a single blob. sig - The actual signature. Another optional but useful field is st , or signedstart , which prevents access until the specified time . Copy the Blob SAS URL and open it using an unauthenticated browser session (e.g., Chrome's Incognito Mode). You should now be able to download this sensitive file without providing any additional credentials! Terminate Cloud Resources and Instance Delete Storage Account Navigate to the storage account, click Delete , enter the storage account name, and click delete: Cancel Subscription To guarantee that additional bills will not accumulate, consider canceling your subscription at the conclusion of the workshop. Alternatively, feel free to keep this account for future testing.","title":"Azure"},{"location":"workshop/azure/#azure","text":"","title":"Azure"},{"location":"workshop/azure/#initialize-an-azure-subscription","text":"Important You must not use a cloud account with existing personal or corporate resources. We will be uploading sensitive files that should not be in production environments. Browse to the Azure Portal . Login to an existing Microsoft account or create a new one: Dismiss any pop-up boxes in the portal until you see a listing of Azure services. Click Start under Start with an Azure free trial and follow all of the prompts until you are back at the portal:","title":"Initialize an Azure Subscription"},{"location":"workshop/azure/#create-a-storage-account-and-upload-the-mock-data","text":"In the Azure Portal, type Storage accounts in the search bar and click the first result under Services : Click Create storage account : Click Create new to create a new resource group and give it any name that you would like. Then, give the storage account a globally unique name, such as sansworkshop**YOUR_NAME** . If the resulting bucket name is more than 24 characters, please abbreviate. Then, click Review : After Azure finishes running its final validation, click Create : Wait for the deployment to complete. Then, click Go to resource : Click Containers : Click + Container , give the container any name that you would like, make sure that Public access level is set to Private (no anonymous access) , and click Create : Click on the newly created container. Then, click Upload , Browse for files and select the MOCK_DATA*.csv files generated earlier: Then, click Upload :","title":"Create a Storage Account and Upload the Mock Data"},{"location":"workshop/azure/#generate-and-use-an-azure-storage-shared-access-signatures-sas","text":"Close the upload file modal. Then, click the ... next to one of the uploaded files and click Generate SAS : Without changing any settings, click **Generate SAS token and URL : Scroll down and view the Blob SAS token . It consists of several components: se ( signedexpiry ) - The date this token will expire . sp ( signedpermissions ) - The permission(s) granted by the token . For blobs, this is one or more of the following: r (Read), a (Add), c (Create), w (Write), d (Delete), and l (List). sv ( signedversion ) - The release date of this version of the storage service . sr ( signedresource ) - The type of resource that can be accessed with this SAS . In this case, it is b to indicate that it is for a single blob. sig - The actual signature. Another optional but useful field is st , or signedstart , which prevents access until the specified time . Copy the Blob SAS URL and open it using an unauthenticated browser session (e.g., Chrome's Incognito Mode). You should now be able to download this sensitive file without providing any additional credentials!","title":"Generate and Use an Azure Storage Shared Access Signatures (SAS)"},{"location":"workshop/azure/#terminate-cloud-resources-and-instance","text":"","title":"Terminate Cloud Resources and Instance"},{"location":"workshop/azure/#delete-storage-account","text":"Navigate to the storage account, click Delete , enter the storage account name, and click delete:","title":"Delete Storage Account"},{"location":"workshop/azure/#cancel-subscription","text":"To guarantee that additional bills will not accumulate, consider canceling your subscription at the conclusion of the workshop. Alternatively, feel free to keep this account for future testing.","title":"Cancel Subscription"},{"location":"workshop/google/","text":"Google Cloud Initialize a Google Cloud Project Important To guarantee that you will not accumulate bills after your free trial concludes, consider shutting down your Google Cloud project at the conclusion of the workshop. You must not use a cloud account with existing personal or corporate resources. We will be uploading sensitive files that should not be in production environments. Navigate to the GCP Console . Create a new GCP account by pressing the Get started for free button. Enter your Google account's (email address or mobile number), press Next , and complete the sign-in process. Note If you do not have a Google account, press the Create Account button to register a new account. You may need to Activate your Google Cloud Platform free trial and accept the Google Cloud Platform Free Trial Terms of Service and fill out the Customer info form to complete the activation process. Create a Storage Bucket and Upload the Mock Data In the Google Cloud console, type Cloud Storage in the search bar and click the first result: Click Create : Give the bucket a globally unique name, such as sans-multicloud-data-workshop-**YOUR_NAME** . If the resulting bucket name is more than 63 characters, please abbreviate. Then, click Create : Click Confirm : In the newly created bucket, click Upload Files and select the MOCK_DATA*.csv files generated earlier: Inspect Data with Google Cloud DLP In the Google Cloud console, type DLP in the search bar and click the first result: Click Enable to enable the Cloud Data Loss Prevention (DLP) API : Click Inspection : Click Create Job and Job Triggers : Give the job any name you want (e.g., Test), set the Storage type to Google Cloud Storage, type the name of your storage bucket in the URL field after the gs:// set the Percentage of included objects scanned within the bucket to 100%, and click Create : Click Confirm Create : Wait until the Pending status is replaced with Done . If desired, you can move onto another cloud provider in the workshop in the meantime. Then, analyze the results. We can make a couple of interesting conclusions based on these results: Our first two spreadsheets contained 2,000 names (1,000 first names and 1,000 last names), but many more were detected . This indicates that there are some false positives. Most critically, Cloud DLP is not flagging ICD-10 Diagnosis Codes as sensitive . While these codes are not considered Personal Health Information (PHI) by itself , coupled with names, this information is highly private. Refine the Job Let us create a custom InfoType to detect the ICD-10 Diagnosis Codes in our second spreadsheet. Create a new job with a different name, repeating all of the same steps, but after setting the Percentage of included objects scanned within the bucket to 100%, click Continue : Click Manage InfoTypes : Click Custom , then Add Custom InfoType : Select the Regex expression Type , set the InfoType to ICD-10 Diagnosis Code , and paste the following ( Source ) into the Regex pattern field: [A-TV-Z][0-9][0-9AB]\\.?[0-9A-TV-Z]{0,4} Then, click Done : Click Done again, then click Create and wait for the results. Did you get the results you expected? Terminate Cloud Resources and Instance Delete Storage Bucket In the Google Cloud Console, return to Cloud Storage, check the checkbox next to the bucket used in this workshop, click Delete , and follow the given instructions: Shut Down Project To guarantee that you will not accumulate bills after your free trial concludes, consider shutting down your Google Cloud project at the conclusion of the workshop. Alternatively, feel free to keep this account for future testing. Exploring Further Google Cloud provides this free demo for Cloud DLP that does not require an active Google Cloud project. Consider giving this a try as well.","title":"Google Cloud"},{"location":"workshop/google/#google-cloud","text":"","title":"Google Cloud"},{"location":"workshop/google/#initialize-a-google-cloud-project","text":"Important To guarantee that you will not accumulate bills after your free trial concludes, consider shutting down your Google Cloud project at the conclusion of the workshop. You must not use a cloud account with existing personal or corporate resources. We will be uploading sensitive files that should not be in production environments. Navigate to the GCP Console . Create a new GCP account by pressing the Get started for free button. Enter your Google account's (email address or mobile number), press Next , and complete the sign-in process. Note If you do not have a Google account, press the Create Account button to register a new account. You may need to Activate your Google Cloud Platform free trial and accept the Google Cloud Platform Free Trial Terms of Service and fill out the Customer info form to complete the activation process.","title":"Initialize a Google Cloud Project"},{"location":"workshop/google/#create-a-storage-bucket-and-upload-the-mock-data","text":"In the Google Cloud console, type Cloud Storage in the search bar and click the first result: Click Create : Give the bucket a globally unique name, such as sans-multicloud-data-workshop-**YOUR_NAME** . If the resulting bucket name is more than 63 characters, please abbreviate. Then, click Create : Click Confirm : In the newly created bucket, click Upload Files and select the MOCK_DATA*.csv files generated earlier:","title":"Create a Storage Bucket and Upload the Mock Data"},{"location":"workshop/google/#inspect-data-with-google-cloud-dlp","text":"In the Google Cloud console, type DLP in the search bar and click the first result: Click Enable to enable the Cloud Data Loss Prevention (DLP) API : Click Inspection : Click Create Job and Job Triggers : Give the job any name you want (e.g., Test), set the Storage type to Google Cloud Storage, type the name of your storage bucket in the URL field after the gs:// set the Percentage of included objects scanned within the bucket to 100%, and click Create : Click Confirm Create : Wait until the Pending status is replaced with Done . If desired, you can move onto another cloud provider in the workshop in the meantime. Then, analyze the results. We can make a couple of interesting conclusions based on these results: Our first two spreadsheets contained 2,000 names (1,000 first names and 1,000 last names), but many more were detected . This indicates that there are some false positives. Most critically, Cloud DLP is not flagging ICD-10 Diagnosis Codes as sensitive . While these codes are not considered Personal Health Information (PHI) by itself , coupled with names, this information is highly private.","title":"Inspect Data with Google Cloud DLP"},{"location":"workshop/google/#refine-the-job","text":"Let us create a custom InfoType to detect the ICD-10 Diagnosis Codes in our second spreadsheet. Create a new job with a different name, repeating all of the same steps, but after setting the Percentage of included objects scanned within the bucket to 100%, click Continue : Click Manage InfoTypes : Click Custom , then Add Custom InfoType : Select the Regex expression Type , set the InfoType to ICD-10 Diagnosis Code , and paste the following ( Source ) into the Regex pattern field: [A-TV-Z][0-9][0-9AB]\\.?[0-9A-TV-Z]{0,4} Then, click Done : Click Done again, then click Create and wait for the results. Did you get the results you expected?","title":"Refine the Job"},{"location":"workshop/google/#terminate-cloud-resources-and-instance","text":"","title":"Terminate Cloud Resources and Instance"},{"location":"workshop/google/#delete-storage-bucket","text":"In the Google Cloud Console, return to Cloud Storage, check the checkbox next to the bucket used in this workshop, click Delete , and follow the given instructions:","title":"Delete Storage Bucket"},{"location":"workshop/google/#shut-down-project","text":"To guarantee that you will not accumulate bills after your free trial concludes, consider shutting down your Google Cloud project at the conclusion of the workshop. Alternatively, feel free to keep this account for future testing.","title":"Shut Down Project"},{"location":"workshop/google/#exploring-further","text":"Google Cloud provides this free demo for Cloud DLP that does not require an active Google Cloud project. Consider giving this a try as well.","title":"Exploring Further"}]}